{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c80614",
   "metadata": {},
   "source": [
    "# Zinc_modelのAD判定を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba6555",
   "metadata": {},
   "source": [
    "## ライブラリ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e60e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "\n",
    "from rdkit import Chem\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e257643",
   "metadata": {},
   "source": [
    "## 前処理・モデル構造再定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1723e94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports/definitions OK\n"
     ]
    }
   ],
   "source": [
    "# ---------- (A) same constants as training ----------\n",
    "MAX_ATOMIC_NUM = 100\n",
    "MAX_DEGREE = 5\n",
    "\n",
    "BOND_TYPES = {\n",
    "    Chem.rdchem.BondType.SINGLE: 0,\n",
    "    Chem.rdchem.BondType.DOUBLE: 1,\n",
    "    Chem.rdchem.BondType.TRIPLE: 2,\n",
    "    Chem.rdchem.BondType.AROMATIC: 3,\n",
    "}\n",
    "NUM_BOND_TYPES = 4\n",
    "\n",
    "MIN_FC, MAX_FC = -2, 2\n",
    "FC_OFFSET = -MIN_FC\n",
    "NUM_FC = (MAX_FC - MIN_FC + 1)\n",
    "\n",
    "HYB_MAP = {\n",
    "    Chem.rdchem.HybridizationType.SP: 0,\n",
    "    Chem.rdchem.HybridizationType.SP2: 1,\n",
    "    Chem.rdchem.HybridizationType.SP3: 2,\n",
    "    Chem.rdchem.HybridizationType.SP3D: 3,\n",
    "    Chem.rdchem.HybridizationType.SP3D2: 4,\n",
    "}\n",
    "HYB_UNKNOWN = 5\n",
    "NUM_HYB = 6\n",
    "\n",
    "\n",
    "def atom_features(atom: Chem.rdchem.Atom) -> torch.Tensor:\n",
    "    atomic_num = min(atom.GetAtomicNum(), MAX_ATOMIC_NUM)\n",
    "    degree = min(atom.GetDegree(), MAX_DEGREE)\n",
    "    aromatic = int(atom.GetIsAromatic())\n",
    "\n",
    "    formal_charge = atom.GetFormalCharge()\n",
    "    formal_charge = max(MIN_FC, min(formal_charge, MAX_FC)) + FC_OFFSET  # 0..4\n",
    "\n",
    "    hyb_idx = HYB_MAP.get(atom.GetHybridization(), HYB_UNKNOWN)\n",
    "\n",
    "    return torch.tensor([atomic_num, degree, aromatic, formal_charge, hyb_idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "def bond_features(bond: Chem.rdchem.Bond) -> torch.Tensor:\n",
    "    bond_type = BOND_TYPES.get(bond.GetBondType(), 0)\n",
    "    conjugated = int(bond.GetIsConjugated())\n",
    "    in_ring = int(bond.IsInRing())\n",
    "    return torch.tensor([bond_type, conjugated, in_ring], dtype=torch.long)\n",
    "\n",
    "\n",
    "def canonicalize_smiles(smi: str) -> Optional[str]:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    return Chem.MolToSmiles(mol, canonical=True)\n",
    "\n",
    "\n",
    "def smiles_to_pyg_discrete_v2(smiles: str):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    x = torch.stack([atom_features(a) for a in mol.GetAtoms()], dim=0)  # [N, 5]\n",
    "\n",
    "    edge_index_list, edge_attr_list = [], []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        bf = bond_features(bond)\n",
    "        edge_index_list += [[i, j], [j, i]]\n",
    "        edge_attr_list  += [bf, bf]\n",
    "\n",
    "    if len(edge_index_list) == 0:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        edge_attr  = torch.empty((0, 3), dtype=torch.long)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "        edge_attr  = torch.stack(edge_attr_list, dim=0)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    data.smiles = smiles\n",
    "    return data\n",
    "\n",
    "\n",
    "def serializable_to_scalers(d: Dict[str, Dict[str, float]]) -> Dict[str, Tuple[float, float]]:\n",
    "    return {k: (float(v[\"median\"]), float(v[\"iqr\"])) for k, v in d.items()}\n",
    "\n",
    "\n",
    "def inverse_robust_scalar(x_scaled: float, med: float, iqr: float) -> float:\n",
    "    return float(x_scaled * iqr + med)\n",
    "\n",
    "\n",
    "# ---------- (B) Model definition (must match training) ----------\n",
    "class EdgeCondLinearLayer(MessagePassing):\n",
    "    def __init__(self, hidden_dim: int, num_bond_types: int):\n",
    "        super().__init__(aggr=\"add\")\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_bond_types = num_bond_types\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(num_bond_types, hidden_dim, hidden_dim))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        self.gru = nn.GRUCell(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, bond_type):\n",
    "        m = self.propagate(edge_index, x=x, bond_type=bond_type)\n",
    "        x_out = self.gru(m, x)\n",
    "        return x_out\n",
    "\n",
    "    def message(self, x_j, bond_type):\n",
    "        bt = bond_type.clamp(0, self.num_bond_types - 1)\n",
    "        W_bt = self.W[bt]  # [E,H,H]\n",
    "        m = torch.bmm(W_bt, x_j.unsqueeze(-1)).squeeze(-1)\n",
    "        return m\n",
    "\n",
    "\n",
    "class MPNNRegressor(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 128, num_layers: int = 3, num_targets: int = 3, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_atomic = nn.Embedding(MAX_ATOMIC_NUM + 1, 64)\n",
    "        self.emb_degree = nn.Embedding(MAX_DEGREE + 1, 16)\n",
    "        self.emb_aroma  = nn.Embedding(2, 8)\n",
    "        self.emb_fc     = nn.Embedding(NUM_FC, 8)\n",
    "        self.emb_hyb    = nn.Embedding(NUM_HYB, 8)\n",
    "\n",
    "        node_in_dim = 64 + 16 + 8 + 8 + 8  # 104\n",
    "        self.node_proj = nn.Linear(node_in_dim, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EdgeCondLinearLayer(hidden_dim=hidden_dim, num_bond_types=NUM_BOND_TYPES)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fp_dim = 1024\n",
    "        self.node_to_fp = nn.Linear(hidden_dim, self.fp_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.fp_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.out = nn.Linear(256, num_targets)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr  = data.edge_attr\n",
    "        batch      = data.batch\n",
    "\n",
    "        atomic_num      = x[:, 0].clamp(0, MAX_ATOMIC_NUM)\n",
    "        degree          = x[:, 1].clamp(0, MAX_DEGREE)\n",
    "        aromatic        = x[:, 2].clamp(0, 1)\n",
    "        formal_charge   = x[:, 3].clamp(0, NUM_FC - 1)\n",
    "        hybridization   = x[:, 4].clamp(0, NUM_HYB - 1)\n",
    "\n",
    "        h = torch.cat([\n",
    "            self.emb_atomic(atomic_num),\n",
    "            self.emb_degree(degree),\n",
    "            self.emb_aroma(aromatic),\n",
    "            self.emb_fc(formal_charge),\n",
    "            self.emb_hyb(hybridization),\n",
    "        ], dim=-1)\n",
    "\n",
    "        h = self.node_proj(h)\n",
    "\n",
    "        bond_type = edge_attr[:, 0].clamp(0, NUM_BOND_TYPES - 1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, edge_index, bond_type)\n",
    "\n",
    "        h_fp = self.node_to_fp(h)\n",
    "        g    = global_add_pool(h_fp, batch)\n",
    "\n",
    "        z = self.bn1(self.fc1(g))\n",
    "        z = F.relu(z)\n",
    "        if self.dropout > 0:\n",
    "            z = F.dropout(z, p=self.dropout, training=self.training)\n",
    "\n",
    "        z = self.bn2(self.fc2(z))\n",
    "        z = F.relu(z)\n",
    "        if self.dropout > 0:\n",
    "            z = F.dropout(z, p=self.dropout, training=self.training)\n",
    "\n",
    "        out = self.out(z)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GenericGNNPredictor:\n",
    "    def __init__(self, ckpt_path: str, device: Optional[torch.device] = None):\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        payload = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        self.dataset_name = payload.get(\"dataset_name\", \"unknown\")\n",
    "        self.model_config = payload[\"model_config\"]\n",
    "        self.preprocess_config = payload[\"preprocess_config\"]\n",
    "        self.scalers = serializable_to_scalers(payload[\"scalers\"])\n",
    "        self.target_cols = list(self.model_config[\"target_cols\"])\n",
    "        self.num_targets = int(self.model_config[\"num_targets\"])\n",
    "\n",
    "        # build model\n",
    "        self.model = MPNNRegressor(\n",
    "            hidden_dim=int(self.model_config[\"hidden_dim\"]),\n",
    "            num_layers=int(self.model_config[\"num_layers\"]),\n",
    "            num_targets=int(self.model_config[\"num_targets\"]),\n",
    "            dropout=float(self.model_config.get(\"dropout\", 0.0)),\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.model.load_state_dict(payload[\"state_dict\"])\n",
    "        self.model.eval()\n",
    "\n",
    "        self.do_canonicalize = bool(self.preprocess_config.get(\"smiles_canonicalize\", True))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_smiles(self, smiles: str) -> Dict[str, Any]:\n",
    "        smi_in = smiles\n",
    "        smi = smiles\n",
    "\n",
    "        if self.do_canonicalize:\n",
    "            smi2 = canonicalize_smiles(smi)\n",
    "            if smi2 is None:\n",
    "                return {\"ok\": False, \"smiles_in\": smi_in, \"error\": \"Invalid SMILES\"}\n",
    "            smi = smi2\n",
    "\n",
    "        data = smiles_to_pyg_discrete_v2(smi)\n",
    "        if data is None:\n",
    "            return {\"ok\": False, \"smiles_in\": smi_in, \"smiles_used\": smi, \"error\": \"Failed to build graph\"}\n",
    "\n",
    "        batch = next(iter(DataLoader([data], batch_size=1, shuffle=False))).to(self.device)\n",
    "        pred_scaled = self.model(batch).detach().cpu().view(-1)\n",
    "\n",
    "        pred_raw_dict = {}\n",
    "        for i, t in enumerate(self.target_cols):\n",
    "            med, iqr = self.scalers[t]\n",
    "            pred_raw_dict[t] = inverse_robust_scalar(float(pred_scaled[i].item()), med, iqr)\n",
    "\n",
    "        return {\"ok\": True, \"smiles_in\": smi_in, \"smiles_used\": smi, \"pred_raw\": pred_raw_dict}\n",
    "\n",
    "\n",
    "def load_predictor_generic(dataset_name: str, save_root: str = \"models\", ckpt_name: str = \"checkpoint.pt\",\n",
    "                          device: Optional[torch.device] = None) -> GenericGNNPredictor:\n",
    "    ckpt_path = os.path.join(save_root, dataset_name, ckpt_name)\n",
    "    return GenericGNNPredictor(ckpt_path=ckpt_path, device=device)\n",
    "\n",
    "\n",
    "print(\"Imports/definitions OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d202b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded predictor for dataset: zinc\n",
      "Targets: ['logP', 'qed', 'SAS']\n",
      "Prediction result:\n",
      "{'ok': True, 'smiles_in': 'CCO', 'smiles_used': 'CCO', 'pred_raw': {'logP': 0.25158007044553754, 'qed': 0.35975909160570496, 'SAS': 2.525134175365807}}\n"
     ]
    }
   ],
   "source": [
    "# Load predictor & sanity check\n",
    "\n",
    "# checkpoint から predictor をロード\n",
    "predictor = load_predictor_generic(\n",
    "    dataset_name=\"zinc\",     # models/zinc/checkpoint.pt\n",
    "    save_root=\"models\",\n",
    "    ckpt_name=\"checkpoint.pt\",\n",
    ")\n",
    "\n",
    "print(\"Loaded predictor for dataset:\", predictor.dataset_name)\n",
    "print(\"Targets:\", predictor.target_cols)\n",
    "\n",
    "# 簡単なSMILESで予測テスト\n",
    "res = predictor.predict_smiles(\"CCO\")  # ethanol\n",
    "\n",
    "print(\"Prediction result:\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e19b01",
   "metadata": {},
   "source": [
    "## グラフ埋め込みを抽出・AD範囲決定・AD判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82256d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def embed_smiles(predictor, smiles: str):\n",
    "    \"\"\"\n",
    "    学習済みGNNから、分子fingerprint g（1024次元）を取得する。\n",
    "    予測ヘッドは通さない。AD用。\n",
    "    \"\"\"\n",
    "    smi = smiles\n",
    "    if predictor.do_canonicalize:\n",
    "        smi2 = canonicalize_smiles(smi)\n",
    "        if smi2 is None:\n",
    "            return None\n",
    "        smi = smi2\n",
    "\n",
    "    data = smiles_to_pyg_discrete_v2(smi)\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    # Batch of 1\n",
    "    batch = next(iter(DataLoader([data], batch_size=1, shuffle=False))).to(predictor.device)\n",
    "\n",
    "    m = predictor.model\n",
    "    m.eval()\n",
    "\n",
    "    # --- forward の readout 直前まで ---\n",
    "    x = batch.x\n",
    "    edge_index = batch.edge_index\n",
    "    edge_attr  = batch.edge_attr\n",
    "    batch_vec  = batch.batch\n",
    "\n",
    "    atomic_num      = x[:, 0].clamp(0, MAX_ATOMIC_NUM)\n",
    "    degree          = x[:, 1].clamp(0, MAX_DEGREE)\n",
    "    aromatic        = x[:, 2].clamp(0, 1)\n",
    "    formal_charge   = x[:, 3].clamp(0, NUM_FC - 1)\n",
    "    hybridization   = x[:, 4].clamp(0, NUM_HYB - 1)\n",
    "\n",
    "    h = torch.cat([\n",
    "        m.emb_atomic(atomic_num),\n",
    "        m.emb_degree(degree),\n",
    "        m.emb_aroma(aromatic),\n",
    "        m.emb_fc(formal_charge),\n",
    "        m.emb_hyb(hybridization),\n",
    "    ], dim=-1)\n",
    "\n",
    "    h = m.node_proj(h)\n",
    "\n",
    "    bond_type = edge_attr[:, 0].clamp(0, NUM_BOND_TYPES - 1)\n",
    "    \n",
    "    for layer in m.layers:\n",
    "        h = layer(h, edge_index, bond_type)\n",
    "\n",
    "    h_fp = m.node_to_fp(h)                 # [N, 1024]\n",
    "    g    = global_add_pool(h_fp, batch_vec)  # [1, 1024]\n",
    "\n",
    "    return g.cpu().view(-1).numpy()         # (1024,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c3a9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num smiles (rows): 399128\n",
      "embedding dim: 1024\n",
      "progress: ok=20000 bad=0\n",
      "progress: ok=40000 bad=0\n",
      "progress: ok=60000 bad=0\n",
      "progress: ok=80000 bad=0\n",
      "progress: ok=100000 bad=0\n",
      "progress: ok=120000 bad=0\n",
      "progress: ok=140000 bad=0\n",
      "progress: ok=160000 bad=0\n",
      "progress: ok=180000 bad=0\n",
      "embedded: 199564 / 399128\n",
      "bad: 0\n",
      "Saved:\n",
      " - models/ad_artifacts/zinc/G_train.npy\n",
      " - models/ad_artifacts/zinc/train_smiles_ok.csv\n",
      " - models/ad_artifacts/zinc/train_smiles_bad.csv\n"
     ]
    }
   ],
   "source": [
    "train_csv_path = 'data/zinc/zinc250k_train_processed.csv'\n",
    "smiles_col = 'smiles'\n",
    "\n",
    "out_dir = \"models/ad_artifacts/zinc\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "g_tmp_path   = os.path.join(out_dir, \"G_train_tmp.npy\")   # まず最大行で確保（後で切り詰め）\n",
    "g_final_path = os.path.join(out_dir, \"G_train.npy\")\n",
    "ok_path      = os.path.join(out_dir, \"train_smiles_ok.csv\")\n",
    "bad_path     = os.path.join(out_dir, \"train_smiles_bad.csv\")\n",
    "\n",
    "# 0) 総行数をざっくり数える（ヘッダ除く）\n",
    "with open(train_csv_path, \"r\") as f:\n",
    "    n_total = sum(1 for _ in f) - 1\n",
    "print(\"num smiles (rows):\", n_total)\n",
    "\n",
    "# 1) 埋め込み次元を決定（先頭チャンクから1個成功するまで探す）\n",
    "# チャンク（分割）で読み込む\n",
    "emb_dim = None\n",
    "head = pd.read_csv(train_csv_path, usecols=[smiles_col], nrows=2000)\n",
    "for smi in head[smiles_col].astype(str).tolist():\n",
    "    g = embed_smiles(predictor, smi)\n",
    "    if g is None:\n",
    "        continue\n",
    "    g = np.asarray(g).reshape(-1)\n",
    "    emb_dim = int(g.shape[0])\n",
    "    break\n",
    "if emb_dim is None:\n",
    "    raise RuntimeError(\"Cannot determine embedding dim: no valid smiles in first 2000 rows.\")\n",
    "print(\"embedding dim:\", emb_dim)\n",
    "\n",
    "# 2) 出力配列を「最大n_total行」でmemmap確保（float32固定）\n",
    "G_out = np.lib.format.open_memmap(\n",
    "    g_tmp_path, mode=\"w+\", dtype=np.float32, shape=(n_total, emb_dim)\n",
    ")\n",
    "\n",
    "# 3) OK/NG を逐次でCSV出力（リストで溜めない）\n",
    "with open(ok_path, \"w\") as f_ok, open(bad_path, \"w\") as f_bad:\n",
    "    f_ok.write(\"smiles\\n\")\n",
    "    f_bad.write(\"smiles\\n\")\n",
    "\n",
    "    write_idx = 0\n",
    "    bad_count = 0\n",
    "\n",
    "    chunksize = 2000  # 安全寄り。速ければ5000~20000に増やしてOK\n",
    "    for chunk in pd.read_csv(train_csv_path, usecols=[smiles_col], chunksize=chunksize):\n",
    "        for smi in chunk[smiles_col].astype(str).tolist():\n",
    "            g = embed_smiles(predictor, smi)\n",
    "            if g is None:\n",
    "                bad_count += 1\n",
    "                f_bad.write(smi + \"\\n\")\n",
    "                continue\n",
    "\n",
    "            g = np.asarray(g, dtype=np.float32).reshape(-1)\n",
    "            if g.shape[0] != emb_dim:\n",
    "                bad_count += 1\n",
    "                f_bad.write(smi + \"\\n\")\n",
    "                continue\n",
    "\n",
    "            G_out[write_idx, :] = g\n",
    "            f_ok.write(smi + \"\\n\")\n",
    "            write_idx += 1\n",
    "\n",
    "        # チャンク単位でフラッシュ＆GC（落ちにくくする）\n",
    "        G_out.flush()\n",
    "        gc.collect()\n",
    "\n",
    "        if (write_idx + bad_count) % 20000 < chunksize:\n",
    "            print(f\"progress: ok={write_idx} bad={bad_count}\")\n",
    "\n",
    "# 4) OK行数ぶんだけ final に切り詰めコピー（チャンクで）\n",
    "G_out.flush()\n",
    "del G_out\n",
    "gc.collect()\n",
    "\n",
    "src = np.load(g_tmp_path, mmap_mode=\"r\")\n",
    "dst = np.lib.format.open_memmap(\n",
    "    g_final_path, mode=\"w+\", dtype=np.float32, shape=(write_idx, emb_dim)\n",
    ")\n",
    "\n",
    "step = 20000\n",
    "for s in range(0, write_idx, step):\n",
    "    e = min(write_idx, s + step)\n",
    "    dst[s:e] = src[s:e]\n",
    "dst.flush()\n",
    "\n",
    "del src, dst\n",
    "gc.collect()\n",
    "\n",
    "# 5) tmp消しても良い（残すならコメントアウト）\n",
    "try:\n",
    "    os.remove(g_tmp_path)\n",
    "except Exception as e:\n",
    "    print(\"warn: failed to remove tmp:\", e)\n",
    "\n",
    "print(\"embedded:\", write_idx, \"/\", n_total)\n",
    "print(\"bad:\", bad_count)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", g_final_path)\n",
    "print(\" -\", ok_path)\n",
    "print(\" -\", bad_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca640e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G_tr shape: (199564, 1024) dtype: float32\n",
      "mu computed\n",
      "inv_cov computed\n",
      "Mahalanobis stats (train):\n",
      "  min: 31.996361389189033\n",
      "  mean: 124.53465056014444\n",
      "  max: 2552.217068137432\n",
      "Mahalanobis stats (train): min/mean/max = 31.996361389189033 124.53465056014444 2552.217068137432\n",
      "thr (top 2.0%): 252.2586172494561\n",
      "train out-rate: 0.020003607865146017\n",
      "Saved:\n",
      " - md_mu.npy / md_inv_cov.npy / md_thr.npy\n",
      " - md_median.npy / md_iqr.npy\n"
     ]
    }
   ],
   "source": [
    "# マハラノビス距離計算、閾値設定（落ちにくい版：mmap + チャンク2パス）\n",
    "\n",
    "ad_dir = \"models/ad_artifacts/zinc\"\n",
    "g_path = os.path.join(ad_dir, \"G_train.npy\")\n",
    "\n",
    "# 1) load embeddings (mmap)\n",
    "G_tr = np.load(g_path, mmap_mode=\"r\")\n",
    "N, D = G_tr.shape\n",
    "print(\"G_tr shape:\", G_tr.shape, \"dtype:\", G_tr.dtype)\n",
    "\n",
    "# 2) robust scaling parameters (median / IQR)  ※結果は(1024,)なので軽い\n",
    "median = np.median(G_tr, axis=0)\n",
    "q1 = np.percentile(G_tr, 25, axis=0)\n",
    "q3 = np.percentile(G_tr, 75, axis=0)\n",
    "iqr = q3 - q1\n",
    "iqr_safe = np.where(iqr == 0.0, 1.0, iqr)\n",
    "\n",
    "# 3) mu をチャンクで計算（G_sを作らない）\n",
    "chunk = 20000  # 余裕あれば 50000 などへ\n",
    "mu = np.zeros(D, dtype=np.float64)\n",
    "\n",
    "for s in range(0, N, chunk):\n",
    "    e = min(N, s + chunk)\n",
    "    Xs = (G_tr[s:e].astype(np.float64) - median) / iqr_safe\n",
    "    mu += Xs.sum(axis=0)\n",
    "mu /= N\n",
    "print(\"mu computed\")\n",
    "\n",
    "# 4) cov をチャンクで計算（Xを作らない）\n",
    "S = np.zeros((D, D), dtype=np.float64)\n",
    "for s in range(0, N, chunk):\n",
    "    e = min(N, s + chunk)\n",
    "    Xs = (G_tr[s:e].astype(np.float64) - median) / iqr_safe\n",
    "    Xc = Xs - mu\n",
    "    S += Xc.T @ Xc\n",
    "cov = S / N\n",
    "\n",
    "# 数値安定化\n",
    "eps = 1e-3\n",
    "cov += eps * np.eye(D, dtype=np.float64)\n",
    "\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "print(\"inv_cov computed\")\n",
    "\n",
    "# 5) Mahalanobis distance をチャンクで計算\n",
    "# percentile が必要なので md_tr を memmap に保存してから percentile を取る\n",
    "md_path = os.path.join(ad_dir, \"md_train.npy\")\n",
    "md_mm = np.lib.format.open_memmap(md_path, mode=\"w+\", dtype=np.float64, shape=(N,))\n",
    "\n",
    "for s in range(0, N, chunk):\n",
    "    e = min(N, s + chunk)\n",
    "    Xs = (G_tr[s:e].astype(np.float64) - median) / iqr_safe\n",
    "    Xc = Xs - mu\n",
    "    md_mm[s:e] = np.einsum(\"bi,ij,bj->b\", Xc, inv_cov, Xc)\n",
    "\n",
    "md_mm.flush()\n",
    "del md_mm\n",
    "gc.collect()\n",
    "\n",
    "md_tr = np.load(md_path, mmap_mode=\"r\")\n",
    "\n",
    "print(\"Mahalanobis stats (train):\")\n",
    "print(\"  min:\", float(md_tr.min()))\n",
    "print(\"  mean:\", float(md_tr.mean()))\n",
    "print(\"  max:\", float(md_tr.max()))\n",
    "\n",
    "# 6) threshold: top 2% as out (same as ZINC)\n",
    "out_percent = 2.0\n",
    "thr = float(np.percentile(md_tr, 100.0 - out_percent))\n",
    "\n",
    "print(\"Mahalanobis stats (train): min/mean/max =\", float(md_tr.min()), float(md_tr.mean()), float(md_tr.max()))\n",
    "print(f\"thr (top {out_percent}%):\", thr)\n",
    "print(\"train out-rate:\", float(np.mean(md_tr > thr)))\n",
    "\n",
    "# 7) save artifacts\n",
    "np.save(os.path.join(ad_dir, \"md_mu.npy\"), mu.astype(np.float32))\n",
    "np.save(os.path.join(ad_dir, \"md_inv_cov.npy\"), inv_cov.astype(np.float32))\n",
    "np.save(os.path.join(ad_dir, \"md_thr.npy\"), np.array([thr], dtype=np.float32))\n",
    "\n",
    "np.save(os.path.join(ad_dir, \"md_median.npy\"), median.astype(np.float32))\n",
    "np.save(os.path.join(ad_dir, \"md_iqr.npy\"), iqr_safe.astype(np.float32))\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" - md_mu.npy / md_inv_cov.npy / md_thr.npy\")\n",
    "print(\" - md_median.npy / md_iqr.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c2f4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST embedded: 24946 / 24946 bad: 0\n",
      "AD-out rate (test): 0.021165717950773672\n",
      "\n",
      "Macro-MAE (raw) by AD label\n",
      "  in : mean = 0.07975466540045895 n = 24418\n",
      "  out: mean = 0.16074348794515753 n = 528\n",
      "\n",
      "Per-target MAE (raw) by AD label\n",
      "  logP | in: 0.1017 | out: 0.2113\n",
      "  qed  | in: 0.0255 | out: 0.0554\n",
      "  SAS  | in: 0.1120 | out: 0.2156\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>logP</th>\n",
       "      <th>qed</th>\n",
       "      <th>SAS</th>\n",
       "      <th>pred_logP</th>\n",
       "      <th>pred_qed</th>\n",
       "      <th>pred_SAS</th>\n",
       "      <th>md</th>\n",
       "      <th>ad_label</th>\n",
       "      <th>mae_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCC1([C@H]([NH3+])c2ccc(F)c(C)c2)CCCC1\\n</td>\n",
       "      <td>3.38752</td>\n",
       "      <td>0.816601</td>\n",
       "      <td>3.559824</td>\n",
       "      <td>3.313812</td>\n",
       "      <td>0.827877</td>\n",
       "      <td>3.486692</td>\n",
       "      <td>132.110855</td>\n",
       "      <td>in</td>\n",
       "      <td>0.052705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COc1ccc(-c2nnc(S[C@H](C(=O)NC(N)=O)C(C)C)n2[C@...</td>\n",
       "      <td>4.01650</td>\n",
       "      <td>0.600510</td>\n",
       "      <td>3.439721</td>\n",
       "      <td>3.901112</td>\n",
       "      <td>0.658551</td>\n",
       "      <td>3.379234</td>\n",
       "      <td>143.705856</td>\n",
       "      <td>in</td>\n",
       "      <td>0.077972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCOc1ccc(NC(=O)N(C)C[C@H]2CCCO2)c(C(F)(F)F)c1\\n</td>\n",
       "      <td>3.74680</td>\n",
       "      <td>0.857830</td>\n",
       "      <td>2.706953</td>\n",
       "      <td>3.704649</td>\n",
       "      <td>0.864768</td>\n",
       "      <td>2.786199</td>\n",
       "      <td>91.791649</td>\n",
       "      <td>in</td>\n",
       "      <td>0.042778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nn1c(SCC(=O)NCCc2ccc(Cl)cc2)nnc1-c1ccc(Cl)cc1\\n</td>\n",
       "      <td>3.41670</td>\n",
       "      <td>0.441151</td>\n",
       "      <td>2.052493</td>\n",
       "      <td>3.633578</td>\n",
       "      <td>0.447660</td>\n",
       "      <td>2.193356</td>\n",
       "      <td>139.434952</td>\n",
       "      <td>in</td>\n",
       "      <td>0.121417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC[C@@]1(c2ccccc2)NC(=O)N(NC(=O)c2cc(C)no2)C1=O\\n</td>\n",
       "      <td>1.48512</td>\n",
       "      <td>0.835476</td>\n",
       "      <td>2.971892</td>\n",
       "      <td>1.195782</td>\n",
       "      <td>0.813572</td>\n",
       "      <td>2.967910</td>\n",
       "      <td>157.378967</td>\n",
       "      <td>in</td>\n",
       "      <td>0.105074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nc1nc(-c2cccc(F)c2)c(Br)s1\\n</td>\n",
       "      <td>3.29390</td>\n",
       "      <td>0.850507</td>\n",
       "      <td>2.322955</td>\n",
       "      <td>3.409121</td>\n",
       "      <td>0.805382</td>\n",
       "      <td>2.371657</td>\n",
       "      <td>77.741875</td>\n",
       "      <td>in</td>\n",
       "      <td>0.069683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O=C1Cc2cc(NC(=O)c3ccccc3C(=O)c3ccccc3)ccc2N1\\n</td>\n",
       "      <td>3.66450</td>\n",
       "      <td>0.681828</td>\n",
       "      <td>1.979296</td>\n",
       "      <td>3.736431</td>\n",
       "      <td>0.673051</td>\n",
       "      <td>1.831247</td>\n",
       "      <td>73.775932</td>\n",
       "      <td>in</td>\n",
       "      <td>0.076252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>COCCCn1c(C(=O)Nc2ccc(C)cc2)cc2c(=O)n3ccccc3nc21\\n</td>\n",
       "      <td>3.24642</td>\n",
       "      <td>0.504632</td>\n",
       "      <td>2.336396</td>\n",
       "      <td>3.286280</td>\n",
       "      <td>0.516607</td>\n",
       "      <td>2.318305</td>\n",
       "      <td>126.285828</td>\n",
       "      <td>in</td>\n",
       "      <td>0.023309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CC1CC[NH+](CN2C(=O)[C@@](O)([C@@H]3SC(N)=NC3=O...</td>\n",
       "      <td>-0.55020</td>\n",
       "      <td>0.684974</td>\n",
       "      <td>4.657024</td>\n",
       "      <td>-0.336659</td>\n",
       "      <td>0.682026</td>\n",
       "      <td>4.913285</td>\n",
       "      <td>222.293198</td>\n",
       "      <td>in</td>\n",
       "      <td>0.157583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CO[C@H]1CN(C(=O)COc2cccc(C)c2C)CC[C@H]1[NH3+]\\n</td>\n",
       "      <td>0.54004</td>\n",
       "      <td>0.898171</td>\n",
       "      <td>3.463921</td>\n",
       "      <td>0.558493</td>\n",
       "      <td>0.863342</td>\n",
       "      <td>3.578106</td>\n",
       "      <td>112.222900</td>\n",
       "      <td>in</td>\n",
       "      <td>0.055822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles     logP       qed  \\\n",
       "0           CCC1([C@H]([NH3+])c2ccc(F)c(C)c2)CCCC1\\n  3.38752  0.816601   \n",
       "1  COc1ccc(-c2nnc(S[C@H](C(=O)NC(N)=O)C(C)C)n2[C@...  4.01650  0.600510   \n",
       "2    CCOc1ccc(NC(=O)N(C)C[C@H]2CCCO2)c(C(F)(F)F)c1\\n  3.74680  0.857830   \n",
       "3    Nn1c(SCC(=O)NCCc2ccc(Cl)cc2)nnc1-c1ccc(Cl)cc1\\n  3.41670  0.441151   \n",
       "4  CC[C@@]1(c2ccccc2)NC(=O)N(NC(=O)c2cc(C)no2)C1=O\\n  1.48512  0.835476   \n",
       "5                       Nc1nc(-c2cccc(F)c2)c(Br)s1\\n  3.29390  0.850507   \n",
       "6     O=C1Cc2cc(NC(=O)c3ccccc3C(=O)c3ccccc3)ccc2N1\\n  3.66450  0.681828   \n",
       "7  COCCCn1c(C(=O)Nc2ccc(C)cc2)cc2c(=O)n3ccccc3nc21\\n  3.24642  0.504632   \n",
       "8  CC1CC[NH+](CN2C(=O)[C@@](O)([C@@H]3SC(N)=NC3=O... -0.55020  0.684974   \n",
       "9    CO[C@H]1CN(C(=O)COc2cccc(C)c2C)CC[C@H]1[NH3+]\\n  0.54004  0.898171   \n",
       "\n",
       "        SAS  pred_logP  pred_qed  pred_SAS          md ad_label  mae_macro  \n",
       "0  3.559824   3.313812  0.827877  3.486692  132.110855       in   0.052705  \n",
       "1  3.439721   3.901112  0.658551  3.379234  143.705856       in   0.077972  \n",
       "2  2.706953   3.704649  0.864768  2.786199   91.791649       in   0.042778  \n",
       "3  2.052493   3.633578  0.447660  2.193356  139.434952       in   0.121417  \n",
       "4  2.971892   1.195782  0.813572  2.967910  157.378967       in   0.105074  \n",
       "5  2.322955   3.409121  0.805382  2.371657   77.741875       in   0.069683  \n",
       "6  1.979296   3.736431  0.673051  1.831247   73.775932       in   0.076252  \n",
       "7  2.336396   3.286280  0.516607  2.318305  126.285828       in   0.023309  \n",
       "8  4.657024  -0.336659  0.682026  4.913285  222.293198       in   0.157583  \n",
       "9  3.463921   0.558493  0.863342  3.578106  112.222900       in   0.055822  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# テストデータでAD評価\n",
    "# ---- load AD artifacts ----\n",
    "ad_dir = \"models/ad_artifacts/zinc\"\n",
    "\n",
    "mu = np.load(os.path.join(ad_dir, \"md_mu.npy\"))\n",
    "inv_cov = np.load(os.path.join(ad_dir, \"md_inv_cov.npy\"))\n",
    "thr = float(np.load(os.path.join(ad_dir, \"md_thr.npy\"))[0])\n",
    "\n",
    "mean  = np.load(os.path.join(ad_dir, \"md_median.npy\"))\n",
    "scale = np.load(os.path.join(ad_dir, \"md_iqr.npy\"))\n",
    "\n",
    "# safety\n",
    "iqr_safe = np.where(scale == 0.0, 1.0, scale)\n",
    "\n",
    "def mahalanobis_distance_from_g(G, mean, scale, mu, inv_cov):\n",
    "    \"\"\"\n",
    "    G: (N, D) raw embedding\n",
    "    standardize -> subtract mu -> mahalanobis\n",
    "    \"\"\"\n",
    "    Gs = (G - mean) / scale\n",
    "    D = Gs - mu.reshape(1, -1)\n",
    "    md = np.einsum(\"bi,ij,bj->b\", D, inv_cov, D)\n",
    "    return md\n",
    "\n",
    "# ---- load test dataframe ----\n",
    "test_csv_path = \"data/zinc/zinc250k_test_processed.csv\"   # ←あなたの実ファイル名に合わせて\n",
    "smiles_col = \"smiles\"\n",
    "\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# 目的変数（raw）列名：あなたのdfに合わせる\n",
    "target_cols = predictor.target_cols  # ['logP', 'qed', 'SAS']\n",
    "\n",
    "# ---- embed test smiles ----\n",
    "smiles_list = test_df[smiles_col].astype(str).tolist()\n",
    "\n",
    "G_list = []\n",
    "ok_idx = []\n",
    "bad_idx = []\n",
    "\n",
    "for i, smi in enumerate(smiles_list):\n",
    "    g = embed_smiles(predictor, smi)\n",
    "    if g is None:\n",
    "        bad_idx.append(i)\n",
    "        continue\n",
    "    G_list.append(g)\n",
    "    ok_idx.append(i)\n",
    "\n",
    "G_te = np.stack(G_list, axis=0)  # (N_ok, 1024)\n",
    "\n",
    "# ---- AD score & label ----\n",
    "md_te = mahalanobis_distance_from_g(G_te, median, iqr_safe, mu, inv_cov)\n",
    "ad_label = np.where(md_te > thr, \"out\", \"in\")\n",
    "\n",
    "print(\"TEST embedded:\", len(ok_idx), \"/\", len(test_df), \"bad:\", len(bad_idx))\n",
    "print(\"AD-out rate (test):\", np.mean(ad_label == \"out\"))\n",
    "\n",
    "# ---- GNN prediction (raw) for same ok samples ----\n",
    "# predictor.predict_smiles は1件ずつで遅いので、ここでは簡易にループで回す（まずは検証優先）\n",
    "pred_rows = []\n",
    "for i in ok_idx:\n",
    "    smi = smiles_list[i]\n",
    "    res = predictor.predict_smiles(smi)\n",
    "    pred_rows.append(res[\"pred_raw\"])\n",
    "\n",
    "pred_df = pd.DataFrame(pred_rows)  # columns: logP,qed,SAS\n",
    "\n",
    "# true values (raw) for ok samples\n",
    "true_df = test_df.iloc[ok_idx][target_cols].reset_index(drop=True)\n",
    "\n",
    "# ---- error metrics ----\n",
    "abs_err = (pred_df[target_cols].values - true_df[target_cols].values)\n",
    "mae_each = np.mean(np.abs(abs_err), axis=0)              # per target\n",
    "mae_macro = np.mean(np.abs(abs_err), axis=1)             # per sample macro-MAE\n",
    "\n",
    "# ---- in/out split ----\n",
    "mask_in = (ad_label == \"in\")\n",
    "mask_out = (ad_label == \"out\")\n",
    "\n",
    "print(\"\\nMacro-MAE (raw) by AD label\")\n",
    "print(\"  in : mean =\", mae_macro[mask_in].mean(), \"n =\", mask_in.sum())\n",
    "print(\"  out: mean =\", mae_macro[mask_out].mean(), \"n =\", mask_out.sum())\n",
    "\n",
    "print(\"\\nPer-target MAE (raw) by AD label\")\n",
    "for j, t in enumerate(target_cols):\n",
    "    print(f\"  {t:4s} | in: {np.mean(np.abs(abs_err[mask_in, j])):.4f} | out: {np.mean(np.abs(abs_err[mask_out, j])):.4f}\")\n",
    "\n",
    "# ---- attach summary table (optional) ----\n",
    "summary = test_df.iloc[ok_idx][[smiles_col] + target_cols].reset_index(drop=True).copy()\n",
    "for t in target_cols:\n",
    "    summary[f\"pred_{t}\"] = pred_df[t].values\n",
    "summary[\"md\"] = md_te\n",
    "summary[\"ad_label\"] = ad_label\n",
    "summary[\"mae_macro\"] = mae_macro\n",
    "\n",
    "display(summary.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
